\chapter{Novel Algorithms}\label{C10}
In this chapter we present in details the novel algorithms and their theoretical guarantees. In the first section we give an overview of the MAB-PR algorithms. Then, we describe the frequentist and the Bayesian policies developed.

%Multi-armed Bandit problems pertain to the Online Learning field,

\section{Introduction to the Novel Algorithms}

As outlined in \cite{banditalgowebopt}, Multi-armed Bandit algorithms have to actively select which data you should acquire and analyze that data in real-time. Indeed, bandit algorithms exemplify two types of learning: \emph{active learning}, which refers to algorithms that actively select which data they should receive; and \emph{online learning}, which refers to algorithms that analyze data in real-time and provide results on the fly. This means that to evaluate algorithms we need to design a simulation environment where we can mimic real-scenarios dynamics. Informally, we call \emph{bucket} a realization $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$ of the persistency vector and we call \emph{bin}  the m-th element  $Z_{j,m,t}$  of the persistency vector $\boldsymbol{Z_{j,t}}$.   During the simulation, each bucket will be parsed according to the experiment time $t$, meaning that the learner can only visit the bin of the bucket containing the information gathered in the past. As stated in Chapter \ref{CF}, when at time $t$ an arm $a_j$ is played, the environment generates a bucket $\boldsymbol{Z_{j,t}}$ and a feedback $R_{j,t}$ that are collected by the learner.
Changing the point of view, we can say that, during the experiment, each arm $a_j$ collects a sequence of pulls, characterized by the extracted feedback-bucket pairs $(R_ {j, t}, \boldsymbol{Z_ {j, t}})$. The algorithms described below have a structural difference from the standard ones designed for traditional Multi-armed Bandit. Indeed, in the presented framework, more than one arm can have a set of non-terminated buckets (not totally parsed yet) simultaneously.  This parallelism implies that at each time instant we need to update the terms of every arms and not only of the last one played. Some of the algorithms presented will work without distinction with both pull reward and normalized pull reward.  For the sake of clarity, we define a generic reward $W_{j,t}$ with unknown expectations $\omega_j$ that acts as a proxy variable. Depending on the setting addressed, $W_{j,t}$ will represent the pull reward $X_{j,t}$ or the normalized pull reward $Y_{j,t}$. The table \ref{algsumm} summarizes the algorithms presented and their operating characteristics.

\subsection{Farsighted and Myopic configuration}

We can consider a bucket $\boldsymbol{Z_{j,t}}$ terminated (fully parsed) according to two different criteria: (i) $\Tmax$ time instants have passed since t; (ii) $l_{j,t}$ time instants have passed since t. We say that the algorithm is in \emph{myopic} and \emph{farsighted} configuration in the case we are adopting (i) or (ii) respectively. More formally:
\begin{itemize}
	\item In \emph{myopic} configuration, at time t, we consider a bucket $\boldsymbol{Z_{j,s}}$ terminated if $t\geq s+\Tmax \ ;$
	\item In \emph{farsighted} configuration, at time t, we consider a bucket $\boldsymbol{Z_{j,s}}$ terminated if $t \geq s+l_{j,s}$, where $l_{j,s}$ is the true length of the bucket $\boldsymbol{Z_{j,s}}$.
	
\end{itemize}

%\\BOZZA:
%\begin{enumerate}
%	
%	\item Baseline UCB 
%	\item Baseline TS
%	\item Bound 1
%	\item Idea 2
%	\item (TS persistent \item TS persistent forced exploartion \item  TS opt)
%	\item Bayes UCB Persistent
%	
%	
%\end{enumerate}


\section{Frequentist Algorithms}
To facilitate the understanding of the code, the algorithms described in this section will be decoupled into two functions: the \emph{Policy} and the \emph{Update}. The policy function will describe the interaction of the learner with the environment and will require an update function passed as an argument. The update function will be responsible to update the knowledge of the learner coming from new data and compute the indices needed by the policy to take decisions, concretizing the overall strategy. The pseudo-code of a generic frequentist policy is depicted in Algorithm \ref{alg:FreqPolicy}.  The first $k$ instants form the initialization phase, during which  each arm is chosen once. After the $k^{th}$ time instant the loop phase begins. Here, at time t, the agent plays the arm $a_j$ having the largest index $u_j$, the upper confidence bound of the arm $a_j$. After the play of an arm, the update function $U$ occurs.
Algorithm \ref{alg:FreqPolicy} requires in input an arm set $A$, the time horizon $N$, and an update function $U$. Below we propose three update functions that can be passed in input to the generic policy depicted in Algorithm \ref{alg:FreqPolicy}, concretizing the following policies: PR-T-UCB, PR-BW-UCB-P, \emph{Idea2}.

\begin{algorithm}[H]
	\caption{\texttt{Frequentist Policy}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, update function ${U}$}			
			%\Statex
			\Function{policy}{\emph{$A$,$N$,$U$}}
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
			\State pull arm $a_t$\;
			\State \emph{call U}\;
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
			\State pull arm $a_i$ such that  $i = \argmax_j$ $u_j $\;
			\State \emph{call U}\;
			\EndFor
			\EndFunction			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:FreqPolicy}
\end{algorithm}



\subsection{PR-T-UCB (Frequentist Baseline Algorithm)}
%This algorithm can be used in both \emph{Tight Persistency} and \emph{General Persistency} configuration.

PR-T-UCB (Persistent Reward - Terminated Buckets - UCB) is an algorithm that extends the idea of the well known UCB algorithm in the case of persistent reward. The reward is considered as a unique variable available after the termination of the bucket. The pseudo-code is provided in Algorithm \ref{alg:BaselineUCB}.
This algorithm takes inspiration from the work of \cite{joulani2013} on delayed feedback. Indeed, evaluating the reward only after termination of the bucket reduces the persistent feedback problem to a delayed feedback problem. For this reason we consider it as a baseline algorithm with which the other proposed strategies are compared. It works for both the cases when we want to maximize the pull reward ($\omega_j = \mu_j$) or when we want to maximize the normalized pull reward ($\omega_j = \eta_j$). In the former we will extended the name of the algorithm to PR-T-UCB-P, in the latter to PR-T-UCB-NP.
For each arm $a_j$, the update function will detect the terminated buckets according to the configuration (farsighted or myopic) adopted. At each time $t$, for each arm $a_j$, the algorithm computes: the empirical mean reward $\hat{\omega}_j(t)$ obtained from the terminated buckets of the arm $a_j$, ignoring the non-terminated buckets (line 3); the exploration term $c_j(t)$ (line 4). Let $\epsilon(t) = \sqrt{\frac{2\ln t}{B_j(t)}} \ $, the exploration term  $c_j(t)$ is computed in the following way.
\begin{itemize}
	\item if we are maximizing the \emph{Pull Reward}:\\
	\begin{centering}
		$c_j(t) = \Rmax  \Tmax \epsilon(t) = \Rmax  \Tmax \sqrt{\frac{2\ln t}{B_j(t)}} \ ;$	
	\end{centering}
		 
	\item if we are maximizing the \emph{Normalized Pull Reward}:\\
	\begin{centering}
			$c_j(t) =\Rmax  \epsilon(t) = \Rmax  \sqrt{\frac{2\ln t}{B_j(t)}} \ ;$
	\end{centering}	
\end{itemize}


 where with ${B_j(t)}$ we indicate the number of terminated buckets of the arm $a_j$ at time $t$. The term $\epsilon(t)$ is the exploration term when the reward has support in [0,1]. With $c_j(t)$ we adapt this term considering the support of the reward in our case.
 
 Finally, the index $u_j(t)$ is computed by summing the current empirical mean reward $\hat{\omega}_j(t) $ and exploration term $c_j(t)$ (line 5). In case an arm $a_j$ does not have any terminated bucket, its index $u_j(t)$ is set to infinite. If we are dealing with settings where the feedback $R_{j,t}$ is deterministic ($R_{j,t}=R_j$ for each arm $a_j$, for each $t$), we can replace $\Rmax$ with $R_j$ in the computation of the exploration term $c_j(t)$. This let us to have a smaller or equal exploration term $c_j(t)$, and possibly, an improvement on the performances.

\begin{algorithm}[H]
	\caption{\texttt{PR-T-UCB (Frequentist Baseline)}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			\Require{bucket size $\Tmax$, maximum feedback $\Rmax$}
			\Function{update}{$\Tmax$,$\Rmax$}
			\For{each arm $a_j \in  A$}			
			\State compute empirical mean reward $\hat{\omega}_j $ from the terminated buckets \;			
			\State compute $c_j$\;
			\State$u_j \gets  \hat{\omega}_j +  c_j$
			\EndFor
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}

\cite{joulani2013} provide the Delayed-UCB1 algorithm, a modification of the UCB algorithm for MAB problem with delayed feedback. Delayed-UCB1 algorithm enjoys the same regret guarantees compared to its non-delayed version, up to an additive penalty depending on the delays. We can rewrite their result, adapting it to our setting with deterministic $R_j$, in the following way:

\begin{theorem}
	For $t \ge 1$, the expected regret of the PR-T-UCB-P algorithm is bounded by
	\[ \mathbb{E}[\mathfrak{R}_t] \le \sum_{i:\Delta_i>0} \Bigg( \frac{8 R_i^2 (\Tmax-\Tmin)^2\ln t }{\Delta_i^2} + 1 + \frac{\pi^2}{3} \Bigg) + \sum_{i=1}^{K} \Delta_i \mathbb{E}[G^*_{i,n}],\]
	where $G^*_{i,n}$ is the maximum number of missing feedbacks from arm $i$ during the first $n$ time steps.
\end{theorem} 
Notice that in the Persistent Reward setting $G^*_{i,n}<\Tmax$ and we consider $\Tmin = 0$, therefore, in the worst case, the regret can be bounded by 
\[ \mathbb{E}[\mathfrak{R}_t] \le \sum_{i:\Delta_i>0} \Bigg( \frac{8 R_i^2 \Tmax^2\ln t}{\Delta_i^2} + 1 + \frac{\pi^2}{3} \Bigg) + \Tmax\sum_{i=1}^{K} \Delta_i.\]





\subsection{PR-BW-UCB-P}
The algorithm PR-BW-UCB-P (Persistent - Bin-Wise - UCB - Pull Reward)  is tailored for scenarios where we want to maximize \emph{Pull Reward} and we have a deterministic feedback $R_j$ for each arm $a_j$. Our goal is to exploit also the information given by the non-terminated buckets using bin-wise upper confidence bounds. The idea behind Algorithm \ref{alg:Bound1} is to estimate the average bin value $\overline{z}_{j,m}(t)$ given by the m-th bins of the buckets gained by pulling the arm $a_j$. To compute $\overline{z}_{j,m}(t)$, the algorithm considers only the buckets gained by $a_j$ that have been already parsed at position $m$ (informally called "available buckets" at line 4). We indicate with $V_{j,m}(t)$ the set of buckets $\boldsymbol{Z_{j,s}}$ that at time $t$ have been already parsed at position $m$. Depending on the configuration adopted, we say that:
\begin{itemize}
	\item In \emph{myopic} configuration  $\boldsymbol{Z_{j,s}} \in V_{j,m}(t) \ if \ t\geq s+m-1$
	\item In \emph{farsighted} configuration  $\boldsymbol{Z_{j,s}} \in V_{j,m}(t) \ if \ t\geq s+m-1 \vee \ t \geq s+l_{j,s} $. As a matter of facts, the farsighted configuration allow us to consider a bucket $\boldsymbol{Z_{j,t}}$ fully parsed after that it has been parsed at position true length $l_{j,t}$. 
\end{itemize}


At each time $t$, for each arm $a_j$, for each position $m$, the algorithm computes $\overline{z}_{j,m}(t)$ and $c_{j,m}(t)$ (line 4 and line 5 respectively). $\overline{z}_{j,m}(t)$ is computed in the following way: $$\overline{z}_{j,m}(t) = \frac{1}{\vert V_{j,m}(t) \vert} \sum_{\boldsymbol{Z_{j,s}} \in V_{j,m}(t)} Z_{j,s,m}\ .$$
Finally, for each arm $a_j$, the upper confidence bound $u_j(t)$ is calculated by multiplying the feedback $R_j$  with the sum, over $m$, of the average bin value $\overline{z}_{j,m}$ and the exploration term  $c_{j,m}(t)$  (line 7). Each element of the summation is upper bounded to 1. This is a help we give to the learner to speed up the learning process. Indeed, this is known a priori considering that each bin has support in [0,1]. 









\begin{algorithm}[H]
	\caption{\texttt{PR-BW-UCB-P}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{bucket size $\Tmax$}
			
			
			\Function{update}{$\Tmax$}
			\For{each arm $a_j \in  A$}
			\For{$m \in \{1, \ldots,\Tmax\}  $}
			
			\State compute $\overline{z}_{j,m}$ from the available buckets \;
			\State$c_{j,m}\gets \sqrt{{2\ln t}/{\vert V_{j,m}(t) \vert}}$\;
			
			\EndFor		
			
			\State $u_j \gets R_j \sum_{m=1}^{\Tmax} \min(1,\overline{z}_{j,m}+c_{j,m}) $\;	
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}

Before giving the theoretical analysis we summarize the notation as follows:

\begin{itemize}
	\item $t$ is the current time
	\item $\bar{z}_{s_i,m,t}$  is the average bin-value 
	\item $c_{s,m,t}$ is an exploration term associated to $\bar{z}_{s_i,m,t}$
	\item $n_{s,m,t}=|V_{j,m}(t)|$ is the number of buckets whose $m$-th element is already parsed
\end{itemize}

Starting from the paper of \cite{auer2002finite} we can rewrite:
\begin{align}
	\ell + \sum_{t=1}^\infty \sum_{s=\ell}^{t-1} \sum_{s_i=\ell}^{t-1} \Biggl\{ R^*\Bigg(\sum_{m=1}^{T_{max}}\bar{z}^*_{s,m,t} + c_{s,m,t}\Bigg) \le R_i\Bigg(\sum_{m=1}^{T_{max}}\bar{z}_{s_i,m,t} + c_{s_i,m,t}\Bigg) \Biggl\} 
\end{align}

Note that $R^*\big(\sum_{m=1}^{T_{max}}\bar{z}^*_{s,m,t} + c_{s,m,t}\big) \le R_i\big(\sum_{m=1}^{T_{max}}\bar{z}^*_{s_i,m,t} + c_{s_i,m,t}\big)$ implies that at least one of the following must hold
\begin{align}
	&R^*(\bar{z}^*_{s,m,t} - \mu^*_m + c_{s,m,t}) \le 0 \quad \forall m=1:T_{max} \label{diseq1}\\
	&R_i(\bar{z}_{s_i,m,t} - \mu_{i,m} - c_{s_i,m,t}) \ge 0 \quad \forall m=1:T_{max} \label{diseq2}\\
	&R^*\sum_{m=1}^{T_{max}}\mu^*_{m} - R_i \sum_{m=1}^{T_{max}}\Bigg( \mu_{i,m} +2c_{s_i,m,t}\Bigg) < 0 \label{diseq3}
\end{align}

For all $m=1:T_{max}$, we bound the probability of event (\ref{diseq1}) using Hoeffding bound and $c_{s,m,t}=\sqrt{\frac{2\ln(t)}{n_{s,m,t}}}$

\begin{align}
	&\mathbf{P}(R^*(\bar{z}^*_{s,m,t} - \mu^*_m + c_{s,m,t}) \le 0) = \nonumber\\
	&\mathbf{P}(\bar{z}^*_{s,m,t} \le \mu^*_m - c_{s,m,t}) \le \nonumber\\
	&e^{-2 n_{s,m,t} c^2_{s,m,t}} = \nonumber\\
	&e^{-2 n_{s,m,t} \frac{2\ln(t)}{n_{s,m,t}}} = \nonumber\\
	&e^{-4\ln(t)} = t^{-4}
\end{align}

Similarly, for all $m=1:T_{max}$, we bound the probability of event (\ref{diseq2}) using Hoeffding bound and $c_{s_i,m,t}=\sqrt{\frac{2\ln(t)}{n_{s_i,m,t}}}$
\begin{align}
	\mathbf{P}(R_i(\bar{z}^*_{s_i,m,t} - \mu^*_m + c_{s_i,m,t}) \le 0) \le t^{-4}
\end{align}

Now we look for a $n_{s_i,t}$ so that inequality (\ref{diseq3}) does not hold. This is equilvalent to find a solution such that:
\begin{align}
	&R^*\sum_{m=1}^{T_{max}}\mu^*_{m} - R_i \sum_{m=1}^{T_{max}}\Bigg( \mu_{i,m} +2\sqrt{\frac{2\ln(t)}{n_{s_i,m,t}}}\Bigg) \ge 0 \label{ineq_mag}
\end{align}

We denote $R^*\sum_{m=1}^{T_{max}}\mu^*_{m} - R_i \sum_{m=1}^{T_{max}} \mu_{i,m}$ by $\tilde{\Delta}_i$. Notice that:
\begin{align}
	&\tilde{\Delta}_i -2R_i\sum_{m=1}^{T_{max}}\sqrt{\frac{2\ln(t)}{n_{s_i,m,t}}} \ge \nonumber\\
	&\tilde{\Delta}_i -2R_i\sum_{m=1}^{T_{max}}\sqrt{\frac{2\ln(t)}{\max{\{0,n_{s_i,t}-m+1\}}}} \ge \\
	&\tilde{\Delta}_i -2R_i\sum_{m=1}^{T_{max}}\sqrt{\frac{2\ln(t)}{\max{\{0,n_{s_i,t}-T_{max}+1\}}}} \ge \\
	&\tilde{\Delta}_i -2R_iT_{max}\sqrt{\frac{2\ln(t)}{\max{\{0,n_{s_i,t}-T_{max}+1\}}}} \\
\end{align}


Assuming that $n_{s_i,t}\ge T_{max}$, we find the following bound for $n_{s_i,t}$:
\begin{align}
	&\tilde{\Delta}_i -2R_iT_{max}\sqrt{\frac{2\ln(t)}{n_{s_i,t}-T_{max}+1}} \ge 0 \nonumber\\
	&n_{s_i,t} \ge T_{max}-1+ \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2}
\end{align}

\textcolor{red}{come cappiamo quando $\max{\{0,n_{s_i,t}-m+1\}}=0$ o $n_{s_i,m,t}=0$? Spiegare}

The bound on the average number of time we pull suboptimal arm $i$ is the following.
\begin{align}
	\mathbb{E}[T_i(n)] &\le \ceil[\Bigg]{T_{max}-1+ \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2}} \nonumber\\ &~~~~+ \sum_{t=1}^\infty \sum_{s=1}^{t-1} \sum_{s_i=\ell}^{t-1} \sum_{m=1}^{\Tmax} \mathbf{P}(R^*(\bar{z}^*_{s,m,t} - \mu^*_m + c_{s,m,t}) \le 0) + \mathbf{P}(R_i(\bar{z}_{s_i,m,t} - \mu_m + c_{s_i,m,t}) \le 0)  \nonumber \\
	&\le T_{max} + \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2} + \sum_{t=1}^\infty \sum_{s=1}^{t-1} \sum_{s_i=\ell}^{t-1} \sum_{m=1}^{\Tmax} 2t^{-4} \nonumber \\
	&\le T_{max} + \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2} + \sum_{t=1}^\infty \sum_{s=1}^{t-1} \sum_{s_i=\ell}^{t-1} 2\Tmax t^{-4} \nonumber \\
	&\le T_{max} + \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2} + \Tmax \frac{\pi^2}{3} \nonumber \\
	&\le \frac{8 R_i^2 T_{max}^2 \ln(t)}{\tilde{\Delta}_i^2} + \Tmax \bigg(1+ \frac{\pi^2}{3} \bigg) \nonumber 
\end{align}

\textcolor{red}{Bound migliore scomponendo la somma ($/(1-\epsilon)^2$)? prova}

\textcolor{red}{Per poter paragonare bound sotto a quello sopra non dovrei aggiungere un $\frac{n-campioni-fino-a-m}{\Tmax}$ linea (30) dentro sommatoria per $m=1$ a $\Tmax$. O comunuque è dovuto a questo il fatto che il bound sia diverso: in un caso non importa quanti campioni sono arrivati fino a m perché sommo tutti i bound non pesati da 1 a $\Tmax$. Nell'altro caso faccio una media unica che quindi è più variabile (es se ho motissimi campioni con pochi samples e rimpiazzo gli zeri, allora è piccola, più di quanto sarebbe fare la somma di tutte le medie dei compioni arriv fino a m)}

\textcolor{red}{Questo algo e quello sotto servono in scenari diversi: se il reward è spostato più verso l'inizio (1111000) meglio questo perché ci da l'idea di come va a scemare, se il reward è estratto sempre con la stessa prob va bene l'altro PD-UCB-C, anche se lo vedo più sensato nel caso normalizzato.}

\subsection{Idea2}
Algorithm \ref{alg:Idea2} is tailored for scenarios where we want to maximize \emph{Pull Reward} and we have a deterministic feedback $R_j$ for each arm $a_j$. With this algorithm we want to exploit all the information of the buckets acquired during the learning process, considering both the terminated and non-terminated ones. To do that, at each time instant t, for each arm $a_j$, we fill the non-terminated buckets with fake realizations (line 5). To fill a non-terminated bucket $\boldsymbol{Z_{j,s}}$ means that we impose each component $Z_{j,s,m}$ not-parsed yet to be equal to a fake realization. At this point, we consider the altered bucket $\boldsymbol{Z_{j,s}}$ fully parsed. We designed two possible strategies to fill the non-terminated buckets: (i)
fill the non-terminated buckets with zeros, having a pessimistic approach; (ii) fill the non-terminated buckets with ones, having an optimistic approach. We accompany the name of the algorithm with the adjective \emph{pessimistic} and \emph{optimistic} in case we are adopting (i) or (ii) respectively. At each time instant t, for each arm $a_j$, the exploration term $c_j$ and the index $u_j$ are computed in the following way (respectively at line 8 and at line 9):
$$c_j = R_j \bigg( \sqrt{\frac{2\Tmax\ln{t}}{n_j(t)}} + \frac{\Tmax(\Tmax-1)}{2n_j(t)} \bigg) \ ,$$ 
$$
u_j = R_j \bigg( \frac{\sum_{s=1}^{t}\sum_{m=1}^{\Tmax}Z_{j,s,m}}{n_j(t)}\bigg)+c_j   = \frac{\sum_{s=1}^{t} X_{j,s}}{n_j(t)} +c_j  ,
$$

where $n_j(t)$ is the number of pulls totalized by the arm $a_j$ at time t. The index $u_j$ is the sum of two terms: (i) the average Pull Reward computed considering all the acquired buckets; (ii) the exploration term  $c_j$. Once $u_j$ has been computed, all the fake realizations are removed from the buckets (line 11).

\begin{algorithm}[H]
	\caption{\texttt{Idea2}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{bucket size $\Tmax$}
			
			
			\Function{policy}{$\Tmax$}
			\For{each arm $a_j \in  A$}
			\For{each bucket $\boldsymbol{Z_{j,s}} $}
			\If{$\boldsymbol{Z_{j,s}}$ is not terminated}
			\State fill $\boldsymbol{Z_{j,s}}$ with fake realizations
			\EndIf
			
			\EndFor		
			\State compute $c_j$\;
			\State compute $u_j$\;
			
			\For{each bucket $\boldsymbol{Z_{j,s}} $}
			\State remove fake realizations from $\boldsymbol{Z_{j,s}}$ if any			
			\EndFor
			
	
							
			\EndFor
			
					
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Idea2}
\end{algorithm}




\section{Bayesian Algorithms}

\subsection{Baseline TS}

\begin{algorithm}[H]
	\caption{\texttt{TS Baseline}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{bucket size $\Tmax$}
			
			
			\Function{policy}{$\Tmax$}
			
			\For{each arm $a_j \in  A$} \Comment{init phase}
				\State $S_j \gets 1$ and $F_j \gets 1$
			\EndFor
			
			\For { $t \in  \{1, \ldots,N\}$}  \Comment{loop phase}
				\For{each arm $a_j \in  A$}
				\State sample $\theta_j$ from the Beta($S_j$,$F_j$) distribution
				\EndFor
			\State pull arm $a_j$ such that $j = \argmax_i \theta_iR_i$
			\For{each arm $a_j \in  A$} \Comment{update phase}
				\For{each bucket $\boldsymbol{Z_{j,s}} $}
				\If {$\boldsymbol{Z_{j,s}} $ is terminated}
				\State compute $\tilde{z}_{j,s}$
				\State Perform a Bernoulli trial with success probability $\tilde{z}_{j,s}$ and observe output r
				\If{ r = 1} \;
				\State $S_j \gets S_j + 1$ \;
				
				\Else
				\State $F_j \gets F_j + 1$ \;
				
				\EndIf
				\EndIf		
			\EndFor
			
			\EndFor
			\EndFor
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:TSBASELINE}
\end{algorithm}

\subsection{Bayes UCB Peristent}



\section{Adaptive version}























































\iffalse
	contenuto...




\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
			\Statex
			\Function{policy}{} 
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
				\State play arm $a_t$\;
				\State $\textsc{update}$
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
				\State play arm $a_z$ such that  $z = argmax_j$ $u_j (t)$\;
				\State $\textsc{update}$
			\EndFor
			\EndFunction
			\Statex
			\Function{update}{}
			\For{each arm $a_j \in  A$}
				\For{each bucket $b \in B_j $}
					\If{$b$ is ended}
						\State$x \gets   R_j * \sum_{m=1}^{Tmax}r_{b,m} $\;
						
						\State$\hat{\mu}_j(t) \gets \frac{\hat{\mu}_j(t-1)*(T_j(t)-1)+x}{T_j(t)}$\;
						\State$B_j \gets B_j \setminus \{b\}$\;	
										
					\EndIf				
				\EndFor			
				
			\State$c_j(t)\gets R_j * Tmax * \sqrt{\frac{2log(t)}{T_j(t)}}$\;
			\State$u_j(t) \gets  \hat{\mu}_j(t) +  c_j(t)$
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}


\section{Bound1}
\begin{itemize}
	\item $v_{j,m}(t)$ is the number of time an arm $a_j$ visited a bucket at instant $m$ up to time $t$
	
\end{itemize}
\begin{algorithm}[!h]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
	
			
			\Function{update}{}
			\For{each arm $a_j \in  A$}
			\For{$m \in {1, \ldots,Tmax}  $}
				
				\State$\hat{\mu}_{j,m}(t)  \gets (\sum_{b \in B_j\, visistable\, up \,to\, m}{}{r_{b,m}})/ v_{j,m}(t)$\;
				\State$c_{j,m}(t)\gets  \sqrt{\frac{2log(t*Tmax^{\frac{1}{4}})}{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j(t) \gets  R_j *\sum_{m=1}^{Tmax} min(1,\hat{\mu}_{j,m}(t)+c_{j,m}(t)) $\;	
			\EndFor
			
	
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}

\fi
