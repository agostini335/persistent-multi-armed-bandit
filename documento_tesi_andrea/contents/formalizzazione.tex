\chapter{Problem Formulation}\label{CF}



In this chapter we formally introduce the Multi-Armed Bandit problem with persistent rewards. After defining all the elements and details necessary to depict the interaction between the learning agent and the environment, we specify the performance measures adopted. Finally, we formalize two real-world scenarios which will be deeply analyzed throughout the thesis.


\section{The MAB-PR problem}

The MAB problem with Persistent Rewards (MAB-PR) that we are going to analyze in this thesis is formalized in this section. Over a finite time horizon, composed of $N$ time instants, at each time instant $t$, an agent must pull (choose) an arm (action) $a_t$ from an arm set $A =\{1, \dots, K\} $. When we pull an arm $a_j$ at time $t$, the environment returns a realization of a random variable $R_{j,t}$ and one of a random vector $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$. The vector $\boldsymbol{Z_{j,t}}$ represents the persistency of the feedback $R_{j,t}$, meaning that each component $Z_{j,t,m}$ describes which fraction of $R_{j,t}$ the agent will collect at the m-step. At each time instant from the pull of an arm, the learner will collect a reward called \emph{instantaneous reward} defined as follows:
\begin{definition}[Instantaneous Reward]
	We define the instantaneous reward achived at time $s$, consequently to the pull of the arm j at time t, as:	
		$$r_{j,t,s} = R_{j,t} Z_{j,t,s-t+1} \ ,$$
where $s \in \{t,\dots,t+T_{max}-1\}$ and $1\leq j \leq K$, $1\leq t \leq N$, $1\leq m \leq T_{max}$.
\end{definition}
By setting the size of the vector $\boldsymbol{Z_{j,t}}$ to $T_{max}$, which is a fixed constant, we impose that the lasting of the feedback can be at most $T_{max}$ steps. According to the scenario we are dealing with, $T_{max}$ will be known or unknown to the agent. We do not have any preliminary knowledge regarding the distributions of $R_{j,t}$ and $\boldsymbol{Z_{j,t}}$, we only know that $R_{j,t}$ has support in $[R_{min},R_{max}]$ and $Z_{j,t,m}$ has support in $[0,1]$. Each realization of $\boldsymbol{Z_{j,t}}$ is characterized by the number of steps that we have to wait before having a positive component $Z_{j,t,m}$. We call this quantity \emph{delay} and we formalize it in the following way:
\begin{definition}[Delay]
	We define the delay of a realization $\boldsymbol{Z_{j,t}}$ as:$$d_{j,t} = \sum_{m=1}^{T_{max}}\mathds{1}_{\{Z_{j,t,m}=0\ \wedge\ \forall k<m\ Z_{j,t,k} = 0\}}\ .$$

\end{definition}
We are also interested in the position of the last positive component of a realization $\boldsymbol{Z_{j,t}}$. This quantity represents the true number of steps that we need to wait to collect all the instantaneous rewards achievable after the pull of an arm. For this reason, we call it \emph{true length} and we define it as follows:
\begin{definition}[True Length]
	We define the true length of a realization $\boldsymbol{Z_{j,t}}$ as:
		$$l_{j,t} = \sum_{m=1}^{T_{max}}\mathds{1}_{\{\exists k \ k\geq m \ \mid \ Z_{j,t,k} > 0\}} \ .$$
\end{definition}




To better suit a variety of scenarios that require a persistence reward framework, we devise two distinct configurations:
\begin{itemize}
	\item \textbf{General Persistency} We do not assume anything regarding $\boldsymbol{Z_{j,t}}$. This configuration turns to be suitable for scenarios in which the instantaneous reward could be missing at a certain instant, $Z_{j,t,m} =0$, and then reappear at a later time. 
	
	\item \textbf{Tight Persistency} We impose that, giving a realization of a persistency vector, every non-zero component must be adjacent. More formally, we say that we are in \emph{Tight Persistency} configuration if for each realization $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$ the following condition holds:
	
	\begin{center}
		
		$\sum_{m=1}^{T_{max}}\mathds{1}_{\{Z_{j,t,m}>0\}} = l_{j,t}-d_{j,t} \ .$

	
	\end{center}
	%\begin{center}
	%	$\nexists i,m,k  \quad i<m<k\quad \text{s.t.}\quad Z_{j,t,i}>0 \wedge Z_{j,t,m}=0 \wedge \ Z_{j,t,k}>0 \quad \forall %i,m,k 	\in (1,\dots,T_{max})$.
	%\end{center}
\end{itemize}







We now present two examples derived from practical cases with the aim to highlight the needs that motivate the two configurations mentioned above. 
\begin{example}[Pricing of a magazine subscription]
	We are the seller of an online magazine that works via subscription. To have access to our service, a new user can stipulate a contract with a fixed duration and monthly fees. We allow to suspend and restart the service at every moment during the contract, simply stopping or making the monthly payments. Intuitively, we think that high prices discourage a continuous usage of the service while low prices could lead to stable subscriptions but with the risk of generating unsatisfactory profit. We are facing the problem of finding the best monthly price to assign at the service to maximize the revenue. This scenario can be directly modelled as a MAB persistent problem in \textbf{General Persistency}. Each arm can be assigned to a specific fee designed as a valid option. When the agent pulls an arm the extracted feedback $R_{j,t}$ will be the price related to that arm. The persistency vector $\boldsymbol{Z_{j,t}}$, on the other side, will capture the adherence of the user to the service and will have a size of $T_{max}$ equal to the number of months of the contract. Every component of the vector will be a Bernoulli variable that takes the value 1 if the user has made the payment for a certain month or the value 0 in the opposite case.
	\label{magazine}
\end{example}
\begin{example}[Medical Trial]
	\label{trial}
	We want to conduct an ethical clinical trial to define which is the best medical treatment for a specific chronic illness. Consider the case where we have two options available, a red pill and a blue one, hence we model them as two arms. Every day the agent must choose which one of the two therapies administer to a new patient on the basis of previous observations. Differently from prior MAB application for this task, in this setting we want to consider also the life quality of a patient in addition to his/her lifespan. For this reason after the treatment administration, a patient is tested every day and an index of his/her health status is computed. We assume that this index is ranging from 0 to 1, where 1 represents a perfect state of health and 0 means that the patient is dead. This scenario could be easily addressed as a MAB persistent problem in \textbf{Tight Persistency} configuration with delay steps equal to zero for each realization of the persistency vector. As a matter of fact, we can set $T_{max}$ at the maximum lifespan possible after the diagnosis of the considered ilness, and we can model every component of the vector $\boldsymbol{ Z_ {j, t}} $ as the health status index. For this scenario, $R_{j,t}$ could be fixed to a constant equal for each arm, letting the role of capturing the reward only to the persistency vector $\boldsymbol{ Z_ {j, t}}$. The Tight Peristency condition holds, as a matter of fact, it does not make sense to have a positive index health status after the death of the patient. 
\end{example}

% metrics
The nature of the presented problem leads us to introduce two definitions of reward achievable pulling an arm. In a straightforward manner, we call \emph{Pull Reward} the the sum of the instantaneous rewards gained thanks to the pull. In both Example \ref{magazine} and Example \ref{trial}, the goal of the learning agent was to find the arm able to maximize this quantity. However, in some scenarios could be reasonable to take in consideration also the time needed to collect all the instantaneous rewards of a pull. In particular, we call \emph{Normalized Pull Reward} the sum of instantaneous rewards divided by the true length of the persistency vector.
This measure is particularly relevant when we consider case studies in which we want to allocate resources and we must take into consideration possible vacant periods, as outlined in Example \ref{cloud}.
Formal definitions of rewards are provided below.


\begin{definition}[Pull Reward]
	We define the pull reward achived pulling the arm j at time t as:
		$$X_{j,t} = \sum_{s=t}^{t+T_{max}-1} r_{j,t,s} \ .$$
\end{definition}

\begin{definition}[Normalized Pull Reward]
	We define the normalized pull reward achived pulling the arm j at time t as:
		$$Y_{j,t} = \dfrac{\sum_{s=t}^{t+T_{max}-1} r_{j,t,s}}{l_{j,t}} \ .$$
\end{definition}

We now give an example of a scenario in which we are interested in maximizing the Normalized Pull Reward.
\begin{example}[Pricing of a Cloud Computing Service]
\label{cloud}
A cloud computing company has a new set of servers at its disposal and is facing the problem of deciding the daily price to rent a server. Once a specific price has been chosen, the company will disclose its offer online and, later, will enter into a contract of a fixed duration with the purchaser. Each day of the contract, the user will pay a fixed cost concerning the rent, in addition to a variable cost related to the resources usage. The company assumes that by publishing an offer with an high price it will take a long time to find a buyer, on the contrary, with a very low price it will immediately be able to rent it but with little profit. In this scenario, we see how the unused server time affects the income, therefore, not only the accumulated reward must be taken into account but also the time necessary to find a buyer. The problem is well modelled in 	\textbf{Tight Persistency}. Each arm $a_j$ is associated to a deterministic daily price $R_{j,t}$ and the delay $d_{j,t}$ of each peristency vector $\boldsymbol{Z_{j,t}}$ will represent the days between the publication of the offer and the stipulation of the contract. Hence, $R_{j,t}$ can be seen as the price of one day of full use of the service, and finally each positive component $Z_{j,t,m}$ will indicate the fraction of $R_{j,t}$ to be daily paid by the user. We are interested in finding the arm that maximizes the \textbf{Normalized Pull Reward}, taking into account also the penalty imposed by the vacant periods.

\end{example}

Successive plays of arm $a_j$ yield pull rewards $X_{j,t_1}, X_{j,t_2},\dots$ which are random variables independent and identically distributed according to an unknown distribution with unknown expectation $\mu_j$.
In the same way, we assume that the normalized pull rewards $Y_{j,t_1}, Y_{j,t_2}, \dots$ are random variables i.i.d. with unknown expectation $\eta_j$. For the sake of simplicity, we will refer to a generic reward of arm $a_j$ at time $t$ as  $X_{j,t}$, to adopt the same notation of the standard MAB literature. However, the definitions stated below will apply evenly to the \emph{Pull reward} and the \emph{Normalized pull Reward}, unless otherwise specified.\\

\begin{table}

\begin{center}
	\caption{Configurations-Rewards scenarios. The combination General-Persistency/Normalized Pull Reward leads to cases that are not of practical interest.}\label{tabNF}
	\begin{tabularx}{0.8\textwidth} { 
			| >{\raggedright\arraybackslash}X 
			| >{\centering\arraybackslash}X 
			| >{\centering\arraybackslash}X | }
		\hline
		  & Pull Reward & Normalized Pull Reward \\
		\hline
		General Persistency  &  Spotify Scenario example 1 &   \\
		\hline
		Tight Persistency  & example 2  & 
			    Rent Scenario example 3
			 \\
		\hline		
	\end{tabularx}	
\end{center}

\end{table}




\subsection{ Performance measures}
% policy
The goal of a learning agent is to maximize its cumulated reward, the pulling strategy adopted to accomplish this task is referred as \emph{policy}. To measure the performance of a policy, we compare its behaviour with the one of a fictitious algorithm, called \emph{Oracle}, which for any horizon of $n$ time steps constantly plays the optimal arm. For this purpose, we introduce the concept of \emph{Regret}.
% sampling regret + pseudo regret
\begin{definition}[Regret]
	The Regret of a policy cumulated after n plays is defined as:
	
		$$R_n=\max_{j = \{1,\dots,k\}} \sum_{t=1}^n{X_{j,t}} - \sum_{t=1}^{n} X_{a_t,t}  \ ,$$
where  $a_t$ is the arm played by the learner at time t and the first term \\\ $\max_{j = \{1,\dots,k\}}\sum_{t=1}^n{X_{j,t}}$ represents the reward cumulated by the Oracle up to time n.

\end{definition}
Since both the rewards and the player's actions are stochastic, we introduce a form of average regret called \emph{pseudo-regret}.

\begin{definition}[Pseudo-Regret]
	The Pseudo-Regret of a policy cumulated after n plays is defined as:
		$$\bm\bar{R}_{n}=n{\mu^{*}}- \sum_{t=1}^{n} \mu_{a_t} \ ,$$
	where  $\mu^{*}=\max_{j = \{1,\dots,k\}} \mu_j$ is the expected reward of the optimal arm and $\mu_{a_t}$ is the expected reward of the arm played at time t.
\end{definition}
For clarity, we explicate the definition of \emph{Normalized-Pseudo Regret} in the following way:
\begin{definition}[Normalized Pseudo-Regret]
	The Normalized Pseudo-Regret of a policy cumulated after n plays is defined as:
	$$\bm\bar{\mathit{NR}}_{n}=n{\eta^{*}}- \sum_{t=1}^{n} \eta_{a_t} \ , $$
	where  $\eta^{*}=\max_{j = \{1,\dots,k\}} \eta_j$ is the expected normalized pull reward of the optimal arm and $\eta_{a_t}$ is the expected normalized pull reward of the arm played at time t.
\end{definition}



The Pseudo-Regret form is more suitable for the purpose of our analysis respect to the Regret. Therefore, in what follows of the thesis we will evaluate the algorithms in terms of Pseudo-Regret and Normalized Pseudo-regret in the case we are considering Pull Reward or Normalized Pull Reward respectively.


%example table

\section{Modeling of real-world scenarios}
\subsection{The Spotify Playlist problem}

Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences \citep{recsys1}. One of the most common problems in recommender systems is the \emph{cold-start} problem. The cold-start problem typically happens when the system does not have any form of data on new users and on
new items \citep{recsys2}. There are two distinct categories of cold start: the product cold start and the user cold starts. The new user case refers to when a new user enrolls in the system and for a certain period of time the recommender has to provide recommendations without relying on the user's past interactions, since none has occurred yet \citep{recsys3}. This problem is particularly important when the recommender is part of the service offered to users, since a user who is faced with recommendations of poor quality might soon decide to stop using the system before providing enough interaction to allow the recommender to understand his/her interests. Spotify is a digital music service which has a great interest in recommender systems. In 2018 it was the organizer of the ACM Conference on recommender systems.\\
We model the problem of recommend a playlist to a new Spotify user as a MAB-PR problem, proposing a new approach to mitigate the cold-start problem.

\subsubsection*{Formulation}
When a new user accesses the system, a playlist is proposed. Subsequently, The user will start the reproduction of the playlist and for each song, in any moment, he/she could decide to skip to the next song till the end of the playlist. We are interested in finding the playlist that maximizes the overall listening time. We model the problem as a MAB-PR problem with the specifics reported below.
\begin{itemize}
	\item Each playlist is a set of 20 songs.
	\item Each arm $a_j$ is associated to a playlist.
	\item The feedback $R_{j,t}$ is fixed to a constant equal for each arm, since we are only interested in finding the playlist with the best persistency.
	\item Based on an official dataset released from Spotify, it is known if a user listened a song for the first 25\%, 50\%, 75\% or 100\% of its duration. This granularity lead us to model each song with four adjacent components of the persistency vector $\boldsymbol{Z_{j,t}}$, where each component $Z_{j,t,m}$ represents a quarter of a song. Each component is a \emph{Bernoulli variable} that takes the value of 1 if the user has listened the song up to that quarter or 0 in the opposite case. The persistency vector will capture the adherence of the user during the playlist, hence its size $T_{max}$ will be equal to  the number of songs of a playlist times the granularity, in this case $T_{max} = 4 \times 20 = 80$. $T_{max}$ is assumed to be known by the learner.  An example of a realization of the persistency vector is provided in figure \ref{bucket_spotify}.
	\item We want to find the playlist that give us the highest listening time, hence we want to maxime the \emph{pull reward}.
	
\end{itemize}

\begin{figure}[h]
	
	\centering
		\begin{tabular}{cccccccccccccccccccc}
			\hline
			\multicolumn{1}{|c}{1} & 1 & 1 & \multicolumn{1}{c|}{1} & 1 & 1 & 0 & \multicolumn{1}{c|}{0} & 1 & 1 & 1 & \multicolumn{1}{c|}{1} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} \\ \hline
			\multicolumn{4}{c}{song 1}                              & \multicolumn{4}{c}{song 2}         & \multicolumn{4}{c}{song 3}         & \multicolumn{4}{c}{song 4}         & \multicolumn{4}{c}{song 5}        
		\end{tabular}



\caption{Example of realization of the persistency vector in the Spotify Playlist Problem. The persistency vector is truncated at the fifth song for visualization purposes. Songs 1 and 3 were listened completely, while song 2 was listened to the 50\% of its duration. Song 4 and Song 5 were skipped entirely.}
\label{bucket_spotify}
\end{figure}






\subsection{The Rental Pricing problem}
A company owns a large number of rooms with the same characteristics. These rooms are rented with fixed duration contracts and monthly fees. Once the contract has been signed, the tenant can choose to stay until the expiration date or to cancel, ending his stay before the expiry date. When the company publishes the rental announcement, it is aware that by setting a high fee for the room, it can have a long vacancy period. Furthermore, a high fee could discourage the tenant from staying until the end of the contract. At the same time, by setting a fee too low, the company could make unsatisfactory profits. The problem of choosing the best fee is modelled as a MAB-PR problem with the specifics reported below.

\begin{itemize}
	\item Each arm $a_j$ is associated to a specific fee designed as a valid option.
	\item The feedback $R_{j,t}$ is set equal to the fee of the arm $a_j$. It is deterministic, meaning that $R_{j,t} = R_j \ \forall t$.  
	\item The persistency vector $\boldsymbol{Z_{j,t}}$ represents the period of time ranging from the publication of the rental announcement to the deadline of the contract. Each component of the vector $Z_{j,t,m}$ is a \emph{Bernoulli variable} that represents a month. It will take the value of 1 if the tenant has made the payment for a certain month or 0 in the opposite case.
	\item Each realization of the persistency vector $\boldsymbol{Z_{j,t}}$ is characterized by a delay $d_{j,t}$ equal to the number of vacant months. The vacant months represent the period between the publication of the announcement and the signing of a new contract.
	\item We define the maximum delay as $d_{max}$ and the maximum duration of a contract as $c_{max}$. The size of the persistency vector $\boldsymbol{Z_{j,t}}$ is $T_{max} = d_{max} + c_{max}$ and it is assumed to be known by the learner.
	\item We want to find the fee that allows us to maximize our profit keeping in consideration the vacant periods, where we don't receive payments. For this purpose, we want to maximize the \emph{normalized pull reward}.
	\item We assume that once a contract is cancelled, it is not possible to re-enter. This implies that we are in \emph{tight persistency}. A realization of the persistency vector $\boldsymbol{Z_{j,t}}$ will be a sequence of zeros representing the delay $d_{j,t}$, followed by a sequence of ones representing the actual  contract, followed by a sequence of zeros with length = $T_{max}-l_{j,t}$. An example is provided in figure \ref{bucket_affitto}.
\end{itemize}
 	


\begin{figure}[h]
	\centering
	
	

		\begin{tabular}{|cccccccccc|}
			\hline
			0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\ \hline
		\end{tabular}
	
	
	
	
	\caption{Example of realization of the persistency vector in the Rental Pricing Problem. In this example $T_{max}=10$, the delay $d=2$, the true length $l=8$. The contract last for six months, since we have six ones. The last two zeros are not relevant, a possible interpretation could be that $c_{max}=6$ and $d_{max}=4$.}
	\label{bucket_affitto}
\end{figure}



