\chapter{Novel Algorithms}\label{C10}




\section{Grounding Concepts}
Multi-armed Bandit problems pertain to Online Learning, which means that in order to evaluate algorithms we need to design a simulation environment where we can mimic real-scenarios dynamics. Informally, we will call \emph{Bucket} a realization $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$ of the persistency vector. During the simulation, each bucket will be parsed according to the experiment time $t$, meaning that the learner can only visit the bucket-cells containing the information of the past. As stated in chapter \ref{CF}, when at time $t$ an arm $a_j$ is played, the environment generates a bucket $\boldsymbol{Z_{j,t}}$ and a feedback $R_{j,t}$ that is collected by the learner.
Changing the point of view, we can say that, during the experiment, each arm $a_j$ collects a sequence of pulls, characterized by the extracted feedback-bucket pairs $(R_ {j, t}, \boldsymbol{Z_ {j, t}})$. We define as $P_j$ the set of pulls of arm $a_j$, hence, when an arm is pulled a pair $(R, \boldsymbol{Z})$ id added to the his set. In order to simplify the notation we will omit the arm and time indices where not needed.\\
The algorithms described below have a structural difference from the standard ones designed for traditional Multi-armed Bandit, infact in the presented framework more than one arms can have a set of active buckets (not totally parsed yet) simultaneously.  This parallelism implies that at each time instant we need to update the terms of every arms and not only of the last one played. In order to facilitate the understanding of the code, the described algorithms will be decoupled into two modules, the \emph{Policy} and the \emph{Update} function. The policy module will describe a generic learner's interaction with the environment and will require an update function passed as an argument. The update function will be responsible to update the data and compute the indices needed by the policy to take decisions, concretizing the overall strategy.\\BOZZA:
\begin{enumerate}
	
	\item Baseline UCB 
	\item Baseline TS
	\item Bound 1
	\item TS persistent 
	\item TS persistent forced exploartion
	\item Bayes UCB Persistent
	
	
\end{enumerate}


\section{Frequentist Algorithms}
The pseudo-code of the Frequentist Policy is depicted in Algorithm \ref{alg:FreqPolicy}.  The first k istants form the initialization phase, during which  each arm is chosen once. After the k-th time istant the loop phase begins. Here the agent plays the arm having the largest index $u_j(t)$, the upper confidence bound of the arm $a_j$ at time t. After the play of an arm, the \emph{Update Procedure} occurs.

\begin{algorithm}[!h]
	\caption{\texttt{Frequentist Policy}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, update function ${U}$}			
			\Statex
			\Function{policy}{\emph{$A$,$N$,$U$}}
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
			\State play arm $a_t$\;
			\State \emph{call U}\;
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
			\State play arm $a_i$ such that  $i = \argmax_j$ $u_j $\;
			\State \emph{call U}\;
			\EndFor
			\EndFunction			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:FreqPolicy}
\end{algorithm}



\subsection{Baseline UCB}
This algorithm \ref{alg:BaselineUCB} extends the idea of the well known UCB algorithm in the case of persistent rewards. The reward is considered as a unique variable available after the termination of the bucket.
\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			\Require{bucket size $T_{max}$, maximum feedback $R_{max}$}
			\Function{update}{$T_{max}$,$R_{max}$}
			\For{each arm $a_j \in  A$}
			\For{each pair $(R,\boldsymbol{Z})$ $\in P_j$}
			\If{$\boldsymbol{Z}$ is ended}
			
			\State compute reward $x$\;
			\State compute average reward $\hat{\mu}_j $\;
			\State$P_j \gets P_j \setminus \{(R,\boldsymbol{Z})\}$\;	
			
			\EndIf				
			\EndFor			
			
			\State$c_j\gets R_{max}  T_{max}  \sqrt{\frac{2log(t)}{T_j(t)}}$\;
			\State$u_j \gets  \hat{\mu}_j +  c_j$
			\EndFor
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}
\\
\subsection{Bound 1}

\begin{algorithm}[!h]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{bucket size $T_{max}$}
			
			
			\Function{update}{$T_{max}$}
			\For{each arm $a_j \in  A$}
			\For{$m \in \{1, \ldots,T_{max}\}  $}
			
			\State compute average persistency $\overline{Z_{j,m}}$ on the available buckets \;
			\State$c_{j,m}\gets  \sqrt{{2log(tT_{max}^{\frac{1}{4}})}/{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j \gets  R_j \sum_{m=1}^{T_{max}} min(1,\overline{Z_{j,m}}+c_{j,m}) $\;	
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}







\section{Bayesian Algorithms}

\section{Farsighted versions}

\section{Adaptive versions}























































\iffalse
	contenuto...




\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
			\Statex
			\Function{policy}{} 
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
				\State play arm $a_t$\;
				\State $\textsc{update}$
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
				\State play arm $a_z$ such that  $z = argmax_j$ $u_j (t)$\;
				\State $\textsc{update}$
			\EndFor
			\EndFunction
			\Statex
			\Function{update}{}
			\For{each arm $a_j \in  A$}
				\For{each bucket $b \in B_j $}
					\If{$b$ is ended}
						\State$x \gets   R_j * \sum_{m=1}^{Tmax}r_{b,m} $\;
						
						\State$\hat{\mu}_j(t) \gets \frac{\hat{\mu}_j(t-1)*(T_j(t)-1)+x}{T_j(t)}$\;
						\State$B_j \gets B_j \setminus \{b\}$\;	
										
					\EndIf				
				\EndFor			
				
			\State$c_j(t)\gets R_j * Tmax * \sqrt{\frac{2log(t)}{T_j(t)}}$\;
			\State$u_j(t) \gets  \hat{\mu}_j(t) +  c_j(t)$
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}


\section{Bound1}
\begin{itemize}
	\item $v_{j,m}(t)$ is the number of time an arm $a_j$ visited a bucket at istant $m$ up to time $t$
	
\end{itemize}
\begin{algorithm}[!h]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
	
			
			\Function{update}{}
			\For{each arm $a_j \in  A$}
			\For{$m \in {1, \ldots,Tmax}  $}
				
				\State$\hat{\mu}_{j,m}(t)  \gets (\sum_{b \in B_j\, visistable\, up \,to\, m}{}{r_{b,m}})/ v_{j,m}(t)$\;
				\State$c_{j,m}(t)\gets  \sqrt{\frac{2log(t*Tmax^{\frac{1}{4}})}{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j(t) \gets  R_j *\sum_{m=1}^{Tmax} min(1,\hat{\mu}_{j,m}(t)+c_{j,m}(t)) $\;	
			\EndFor
			
	
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}

\fi
