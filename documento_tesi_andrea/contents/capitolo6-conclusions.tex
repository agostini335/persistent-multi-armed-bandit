\chapter{Conclusions}\label{C6}
The Multi-Armed Bandit model is an important framework for decision making under uncertainty, as it presents one of the clearest examples of the trade-off between exploration and exploitation. In problems of this type, an agent, at each time instant, is required to choose an action among a given set of actions. Each action, when chosen, generates a reward for the agent. The goal of the agent is to maximize the cumulative reward over a finite time horizon. Although the standard bandit model assumes that the reward is a real number immediately available after an action has been taken, this assumption is inappropriate for a variety of scenarios. In this thesis, motivated by real-world application needs, we studied the specific case in which the reward deriving from an action is spread over the time following the taking of the action, naming this setting persistent reward. Applications that fall under persistent reward include, for example, dynamic pricing, web content optimization, recommender systems and adaptive clinical trials.

We firstly formalized a new bandit model, namely Multi-Armed Bandit with Persistent Reward (MAB-PR), suitable to handle persistent reward scenarios. Then, we designed a wide family of algorithms, which includes both Bayesian and Frequentist algorithms, tailored to tackle this bandit model. Our major interest was to develop and analyze novel algorithms able to exploit the partial information obtained during the reward acquistion process. At this purpose, we introduced two different approaches: the bin-wise approach and the non-terminated approach. The former takes advantage of the fact that the total reward deriving from an action can be seen as a sum of smaller rewards (bins) acquired at each time instants after the take of the action. On the other hand, the latter, exploits the not yet terminated reward acquisition processes considering them terminated by faking the part of the process not yet occurred with fictitious information. The algorithms that base their operating criteria behind these approaches have been compared with those that evaluate the reward as a unique variable available only at the end of the acquisition process that, for this reason, we considered as baselines. The baselines algorithms, which do not exploit partial information, come from the Delayed MAB literature. Indeed, to consider the reward as a number available after that the acquisition process is terminated, reduces the persistent reward problem to a delayed reward problem.

For the Frequentist algorithms we provided theoretical guarantees which, under certain conditions, state that both the baseline and the bin-wise algorithm PR-BW-UCB-P achieve a regret bound of the order of $O(\Tmax^2\ln t)$. Furthermore, we proved that the algorithm PR-NT-UCB-P, which relies on the non-terminated approach, achieves a regret bound of the order of $O(\Tmax\ln t)$.

We performed a thorough experimental analysis of the developed algorithms which includes both synthetic-data settings and real-data settings. The experimental results show that, in the large majority of the settings analyzed, the novel algorithms that exploit partial information achieved better results compared to the baselines pertaining to the same group (Bayesian and Frequentist). This is satisfactory since the baseline algorithms represent the state-of-the-art to tackle the addressed problems. Furthermore, the theoretical results are perfectly reflected in the outcome of the experimental analysis, where, for each addressed scenario, the algorithm PR-NT-UCB-P outperforms all the other Frequentist algorithms when a sufficient time horizon is provided. Experimental evidence shows that the way the reward is distributed over the time severely affects the performance of the algorithms. This is actually not surprising. Indeed, it is reasonable to think that if a large portion of the total reward generated by an action is concentrated immediately after that the action has taken place, the advantage of capturing this information without waiting is remarkable. On the contrary, if the reward is uniformly distributed over the timespan following the take of an action, the advantage gathered from an early evaluation is limited. In the worst case, it can even be misleading to exploit partial information if, in an initial phase, these are not representative of the total reward deriving from the take of an action. In our analysis, the Bayesian algorithms always achieve better performance compared to the Frequentist ones. We show experimentally that the Bayesian way to manage the exploration exploitation trade-off is the best in the persistent reward scenario. This confirms what we expected since previous works shows that Thompson Sampling, which inspired our Bayesian algorithms, is more robust under delayed feedback thanks to its randomness \citep{Chapelle2011Thompson}.


The main future directive of this work is to prove theoretical bounds for all the algorithms which have not been sufficiently theoretically analyzed, in particular the ones pertaining to the Bayesian family. Our experimental results emphasised how the persistent reward settings is sensible to the the way in which the reward is distributed along its acquisition process. An interesting research line could be to theoretically formalize this aspect and to devise new strategies which consider it. Alternatively, could be also interesting to prove criteria that, based on the reward distribution along its acquisition process, determine when it is convenient to approach a specific persistent reward scenario with an algorithm that exploits partial information. Other possible extensions can be achieved by combining other MAB variants with the MAB-PR model. In particular, we believe that the two most limiting assumption are: (i) the absence of side information; and (ii) the stationarity of the reward. Indeed, these two assumptions are very significant in a lot of real problems that could be modelled as MAB-PR problem. For example, in a recommendation task, the absence of side information means that the algorithm considers that all the users to be served are identical, or, in a dynamic pricing problem, the stationarity of the reward implies that the algorithm is not able to react to the market variations. Indeed, could be interesting to mix our MAB-PR model with Contextual Bandits \citep{contextual} or Seasonal Bandits \citep{di2020linear} to cope with (i) and (ii) respectively.



