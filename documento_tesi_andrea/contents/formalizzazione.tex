\chapter{Problem Formulation}\label{C10}



Standard MAB problem formulations...

\section{The MAB Persistent problem}

The Mab Persistent problem that we are going to analyze in this thesis is formalized in this section. Over a finite time horizon, composed of $N$ time instants, at each time instant $t$, an agent must pull (choose) an arm (action) $a_t$ from an arm set $A =\{1, \dots, K\} $. When we pull an arm $a_j$ at time $t$ the environment returns a realization of a random variable $R_{j,t}$ and one of a random vector $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$. The vector $\boldsymbol{Z_{j,t}}$ represtens the persistency of the feedback $R_{j,t}$, meaning that each component $Z_{j,t,m}$ describes which fraction of $R_{j,t}$ the agent will collect at the m-step. At each time instant from the pull of an arm, the learner will collect a reward called \emph{instant reward} defined as follows:
\begin{definition}[Instant Reward]
	We define the instant reward achived at time $s$, consequently to the pull of the arm j at time t as:	
		$$r_{j,t,s} = R_{j,t} Z_{j,t,m}$$
Where $s \in \{t,\dots,t+T_{max}-1\}$ and $m = s-t+1$
\end{definition}
By setting the size of the vector $\boldsymbol{Z_{j,t}}$ to $T_{max}$, which is a fixed constant, we impose that the lasting of the feedback can be at most $T_{max}$ steps. In according to the scenario we are dealing with, $T_{max}$ will be known or not by the agent. We do not have any preliminary knowledge regarding the distributions of $R_{j,t}$ and $\boldsymbol{Z_{j,t}}$, we only know that $R_{j,t}$ has support in $[R_{min},R_{max}]$ and $Z_{j,t,m}$ has support in $[0,1]$ where $1\leq j \leq K$, $1\leq t \leq N$, $1\leq m \leq T_{max}$. Each realization of $\boldsymbol{Z_{j,t}}$ is characterized by the number of steps that we have to wait before having a positive component $Z_{j,t,m}$. We call this quantity \emph{delay} and we formalize it in the following way:
\begin{definition}[Delay]
	We define the delay of a realization $\boldsymbol{Z_{j,t}}$ as:$$d_{j,t} = \sum_{m=0}^{T_{max}}\mathds{1}_{\{Z_{j,t,m}=0\ \wedge\ \forall k<m\ Z_{j,t,k} = 0\}}$$

\end{definition}
We are also interested in the position of the last positive component of a realization $\boldsymbol{Z_{j,t}}$. In essence, this quantity represents the true number of steps that we need to wait in order to collect all the instant rewards achievable after the pull of an arm. For this reason we call it \emph{true length} and we define it as follows:
\begin{definition}[True Length]
	We define the true length of a realization $\boldsymbol{Z_{j,t}}$ as:
		$$l_{j,t} = \sum_{m=0}^{T_{max}}\mathds{1}_{\{\exists k \ k\geq m \ \mid \ Z_{j,t,k} > 0\}}$$
\end{definition}




To better suit a variety of scenarios that require a persistence reward framework, we devise two distinct configurations:
\begin{itemize}
	\item \textbf{General Persistency} We do not assume anything regarding $\boldsymbol{Z_{j,t}}$. This configuration turns to be suitable for scenarios in which the istant reward could be missing at a certain point and then reappear at a later time. 
	
	\item \textbf{Tight Persistency} We impose that, giving a realization of a persistency vector, every non-zero component must be adjacent. More formally, we say that we are in \emph{Tight Persistency} configuration if for each realization $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$ it holds the following condition:
	\begin{center}
		$\nexists i,m,k  \quad i<m<k\quad \text{s.t.}\quad Z_{j,t,i}>0 \wedge Z_{j,t,m}=0 \wedge \ Z_{j,t,k}>0 \quad \forall i,m,k 	\in (1,\dots,T_{max})$
	\end{center}
\end{itemize}







We now present a couple of examples derived from practical cases with the aim to highlight the needs that motivate the two configurations above mentioned. 
\begin{example}
	We are the seller of an online magazine that works via subscription. In order to have access to our service, a new user can stipulate a contract with fixed duration and monthly fee. We let the possibility to suspend and restart the service in every moment during the contract simply stopping or making the monthly payments. Intuitively, we think that high prices discourage a continuous usage of the service while low prices could lead to stable subscriptions but with the risk of generating unsatisfactory profit. We are facing the problem of finding the best monthly price to assign at the service in order to maximize the revenue. This scenario can be directly modeled as a MAB persistent problem in \textbf{General Persistency}. Each arm can be assigned to a specific fee designed as a valid option. When the agent pulls an arm the feedback extracted $R_{j,t}$ will be simply the price related to that arm. The persistency vector $\boldsymbol{Z_{j,t}}$, on the other side, will capture the adherence of the user to the service and will have a size of Tmax equal to the number of months of the contract. Every component of the vector will be a Bernoulli variable that takes the value 1 if the user has made the payment in a certain month, 0 on the contrary.
\end{example}
\begin{example}
	We want to conduct an ethical clinical trial, In order to define which is the best medical treatment for a specific chronic illness. Let's say that we have two options available, a red pill and a blue one, hence we model them as two arms. Every day the agent must choose which one of the two therapies administer to a new patient on the basis of previous observations. Different from prior MAB application for this task, here we want to consider also the life quality of a patience in addition to his lifespan. For this reason after the treatment administration, a patient is tested every day and an index of his health status is computed. We can assume that this index is ranging from 0 to 1, where 1 represents a perfect state of health whereas 0 means that the patient is dead. This scenario could be easily addressed as a MAB persistent problem in \textbf{Thigh Persistency} configuration with delay steps equal to zero for each realization of the persistency vector. In fact, we can set $T_{max}$ at the maximum lifespan possible after the diagnosis of the considered ilness, and we can model every component of the vector $\boldsymbol{ Z_ {j, t}} $ as the health status index. For this scenario $R_{j,t}$ could be fixed to a constant equal for each arm, letting the role of capturing the reward only to the persistency vector $\boldsymbol{ Z_ {j, t}}$. The Thigh Peristency condition holds, as a matter of fact, it does not make sense to have a positive index health status after a death. 
\end{example}

% metrics
The nature of the presented problem lead us to introduce two definitions of reward achievable pulling an arm. In a straightforward manner, we call \emph{Pull Reward} the the sum of the instant rewards gained thanks to the pull. In some scenarios could be reasonable to take in consideration also the time needed to collect all the instant rewards of a pull. Hence, we call \emph{Normalized Pull Reward} the sum of instant rewards divided by the true length of the persistency vector. Formal definitions are provided below.


\begin{definition}[Pull Reward]
	We define the pull reward achived pulling the arm j at time t as:
		$$X_{j,t} = \sum_{s=t}^{t+T_{max}-1} r_{j,t,s}$$
\end{definition}

\begin{definition}[Normalized Pull Reward]
	We define the normalized pull reward achived pulling the arm j at time t as:
		$$Y_{j,t} = \dfrac{\sum_{s=t}^{t+T_{max}-1} r_{j,t,s}}{l_{j,t}}$$
\end{definition}

For the sake of simplicity, we will refer to a generic reward of arm $a_j$ at time $t$ as $X_{j,t}$, in order to adopt the same notation of the standard MAB literature. However the definitions stated below will apply evenly to the \emph{Pull reward} or the \emph{Normalized pull Reward}, unless otherwise specified.\\
Successive plays of arm $a_j$ yield rewards $X_{j,t_1}, X_{j,t_2},\dots$ which are random variables independent and identically distributed according to an unknown distribution with unknown expectation $\mu_j$.\\ 
\begin{center}
	\begin{tabularx}{0.8\textwidth} { 
			| >{\raggedright\arraybackslash}X 
			| >{\centering\arraybackslash}X 
			| >{\raggedleft\arraybackslash}X | }
		\hline
		  & Pull Reward & Normalized Pull Reward \\
		\hline
		General Persistency  & example 1 Spostify Scenario  &   \\
		\hline
		Tight Persistency  & example 2  & Rent Scenario  \\
		\hline
	\end{tabularx}
\end{center}



\subsection{ Performance measures}
% policy
The goal of a learning agent is to maximize its cumulated reward, the pulling strategy adopted to accomplish this task is referred as \emph{policy}. In order to measure the performance of a policy, we compare its beahviour with the one of a fictitious algorithm, called \emph{Oracle}, which constantly play the optimal arm. For this purpose we introduce de concept of \emph{Regret}.
% sampling regret + pseudo regret
\begin{definition}[Regret]
	The Regret of a policy cumulated after n plays is definied as:
	
		$$R_n=\max_{j = \{1,\dots,k\}} \sum_{t=1}^n{X_{j,t}} - \sum_{t=1}^{n} X_{a_t,t} $$

	Where  $a_t$ is the arm played by the learner at time t and the first term $\max_{j = \{1,\dots,k\}}\sum_{t=1}^n{X_{j,t}}$ represtents the reward cumulated by the Oracle up to time n.

\end{definition}
Since both the rewards and the player's actions are stochastic, we introduce a form of average regret, called \emph{pseudo-regret}.

\begin{definition}[Pseudo-Regret]
	The Pseudo-Regret of a policy cumulated after n plays is definied as:
		$$\bm\bar{{R_n}}=n{\mu^{*}}- \sum_{t=1}^{n} \mu_{a_t}$$
	Where  $\mu^{*}=\max_{j = \{1,\dots,k\}} \mu_j$ is the expected reward of the optimal arm and $\mu_{a_t}$ is the expected reward of the arm played at time t.
\end{definition}
This form of regret is more suitable for the purpose of our analysis, therefore in the rest of thesis we will evaluate our algorithm in term of pseudo-regret and, for the sake of simplicity, from now on we will refer to it as regret.


%example table





