\chapter{Novel Algorithms}\label{C10}


%Multi-armed Bandit problems pertain to the Online Learning field,

\section{Grounding Concepts}

Multi-armed bandit algorithms have to actively select which data you should acquire and analyze that data in real-time. Indeed, as outlined in \cite{banditalgowebopt}, bandit algorithms exemplify two types of learning: \emph{active learning}, which refers to algorithms that actively select which data they should receive; and \emph{online learning}, which refers to algorithms that analyze data in real-time and provide results on the fly. This means that to evaluate algorithms we need to design a simulation environment where we can mimic real-scenarios dynamics. Informally, we will call \emph{bucket} a realization $\boldsymbol{Z_{j,t}}= (Z_{j,t,1},\dots, Z_{j,t,Tmax})$ of the persistency vector. During the simulation, each bucket will be parsed according to the experiment time $t$, meaning that the learner can only visit the element of the bucket containing the information gathered in the past. As stated in Chapter \ref{CF}, when at time $t$ an arm $a_j$ is played, the environment generates a bucket $\boldsymbol{Z_{j,t}}$ and a feedback $R_{j,t}$ that is collected by the learner.
Changing the point of view, we can say that, during the experiment, each arm $a_j$ collects a sequence of pulls, characterized by the extracted feedback-bucket pairs $(R_ {j, t}, \boldsymbol{Z_ {j, t}})$. We define as $P_j$ the set of pulls of arm $a_j$, hence, when an arm is pulled a pair $(R, \boldsymbol{Z})$ id added to the his/her set. To simplify the notation we will omit the arm and time indices where not needed.\\
The algorithms described below have a structural difference from the standard ones designed for traditional Multi-armed Bandit. Indeed, in the presented framework, more than one arms can have a set of non-terminated buckets (not totally parsed yet) simultaneously.  This parallelism implies that at each time instant we need to update the terms of every arms and not only of the last one played. To facilitate the understanding of the code, the described algorithms will be decoupled into two functions, the \emph{Policy} and the \emph{Update}. The policy function will describe the learner's interaction with the environment and will require an update function passed as an argument. The update function will be responsible to update the knowledge of the learner coming from new data and compute the indices needed by the policy to take decisions, concretizing the overall strategy.
%\\BOZZA:
%\begin{enumerate}
%	
%	\item Baseline UCB 
%	\item Baseline TS
%	\item Bound 1
%	\item Idea 2
%	\item (TS persistent \item TS persistent forced exploartion \item  TS opt)
%	\item Bayes UCB Persistent
%	
%	
%\end{enumerate}


\section{Frequentist Algorithms}
The pseudo-code of a generic frequentist policy is depicted in Algorithm \ref{alg:FreqPolicy}.  The first $k$ istants form the initialization phase, during which  each arm is chosen once. After the $k^{th}$ time istant the loop phase begins. Here the agent plays the arm having the largest index $u_j(t)$, the upper confidence bound of the arm $a_j$ at time t. After the play of an arm, the update function $U$ occurs.
Algorithm \ref{alg:FreqPolicy} requires in input an arm set $A$, the time horizon $N$, and an update function $U$. Below we propose three update functions that can be passed in input to the generic policy depicted in Algorithm \ref{alg:FreqPolicy}, concretizing the following policies: \emph{Baseline UCB}, \emph{Bound 1}, \emph{Idea2}.

\begin{algorithm}[H]
	\caption{\texttt{Frequentist Policy}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, update function ${U}$}			
			%\Statex
			\Function{policy}{\emph{$A$,$N$,$U$}}
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
			\State play arm $a_t$\;
			\State \emph{call U}\;
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
			\State play arm $a_i$ such that  $i = \argmax_j$ $u_j $\;
			\State \emph{call U}\;
			\EndFor
			\EndFunction			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:FreqPolicy}
\end{algorithm}



\subsection{Baseline UCB}
%This algorithm can be used in both \emph{Tight Persistency} and \emph{General Persistency} configuration.

Algorithm \ref{alg:BaselineUCB} extends the idea of the well known UCB algorithm in the case of persistent rewards. The reward is considered as a unique variable available after the termination of the bucket. This algorithm takes inspiration from the work of \cite{joulani2013} on delayed feedback. Indeed, evaluating the reward only after termination of the bucket reduces the persistent feedback problem to a delayed feedback problem. For this reason we consider it as a baseline algorithm with which the other proposed strategies are compared. For each arm $a_j$, the update function will detect the terminated buckets from its set of pulls $P_j$. Depending on the scenario, the algorithm can consider a bucket $\boldsymbol{Z_{j,t}}$ terminated according to two different criteria: (i) $T_{max}$ time instants have passed since t; (ii) $l_{j,t}$ time instants have passed since t. We will accompany the name of the algorithm with the adjective \emph{myopic} and \emph{farsighted} in the case we are adopting (i) or (ii) respectively. When a terminated bucket is detected from $P_j$,
the algorithm computes the average reward $\hat{\mu}_j $ obtained from arm $j$ so far (line 5). At each time t, for each arm $a_j$, the exploration term $c_j$ is computed (line 9), where with ${T_j(t)}$ we indicate the number of terminated bucket of the arm $a_j$ up to time t. After that, The index $u_j$ is computed by summing the current average reward $\hat{\mu}_j $ and exploration term $c_j$ (line 10). In case an arm $a_j$ does not have any terminated bucket, its index $u_j$ is set to infinite. % according to the results obtained in Chapter(??)

\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			\Require{bucket size $T_{max}$, maximum feedback $R_{max}$}
			\Function{update}{$T_{max}$,$R_{max}$}
			\For{each arm $a_j \in  A$}
			\For{each pair $(R,\boldsymbol{Z})$ $\in P_j$}
			\If{$\boldsymbol{Z}$ is terminated}
			
			\State compute average reward $\hat{\mu}_j $\;
			%\State$P_j \gets P_j \setminus \{(R,\boldsymbol{Z})\}$\;	
			
			\EndIf				
			\EndFor			
			
			\State$c_j\gets R_{max}  T_{max}  \sqrt{\frac{2ln(t)}{T_j(t)}}$\;
			\State$u_j \gets  \hat{\mu}_j +  c_j$
			\EndFor
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}

\subsection{Bound 1}
This algorithm is tailored for scenarios where we want to maximize \emph{Pull Reward} and we have a deterministic feedback $R_j$ for each arm $a_j$. The idea behind Algorithm \ref{alg:Bound1} is to estimate, for each arm $a_j$, the average instaneous reward $\overline{r}_{j,m}$ gathered at each position $m \in \{1, \ldots,T_{max}\}$ over the acquired buckets.
To compute $\overline{r}_{j,m}$ at time t, the algorithm considers only the buckets of $a_j$ that have been already visited at position $m$ (informally called "available buckets" at line 4). We indicate with $V_{j,m}(t)$ the set of buckets $\boldsymbol{Z_{j,s}}$ that at time t, have been already visited at position $m$. More formally,  $\boldsymbol{Z_{j,s}} \in V_{j,m}(t) \ if \ t>s+m-1$. At each time t, for each arm $a_j$, for each position $m$, the algorithm compute $\overline{r}_{j,m}$ and $c_{j,m}$ (line 4 and line 5 respectively). $\overline{r}_{j,m}$ is computed in the following way: $$\overline{r}_{j,m} = \frac{1}{\vert V_{j,m}(t) \vert} \sum_{\boldsymbol{Z_{j,s}} \in V_{j,m}(t)} R_jZ_{j,s,m}\ .$$
Finally, for each arm $a_j$, the upper confidence bound $u_j$ is calculated by summing up, for each $m$, the average instaneous reward $\overline{r}_{j,m}$ and the exploration term  $c_{j,m}$  (line 7). Each element of the summation is upper bounded to $R_j$. This is an help we give to the learner to speed up the learning process. Indeed, it is known a priori, considering the formulation of the problem, that the maximum instantaneous reward that can be achieved by pulling an arm $a_j$ with deterministic feedback $R_j$ is $r_{max} = R_j$. With this algorithm we are able to exploit partial information of a bucket without the need to wait its termination. The ver






%Per calcolare rjm al tempo t, l'algoritmo considera solamente i bucket di aj che sono stati visitati in posizione m (indicati come "available buckets" at line 4). Indichiamo con Aj,m(t) il set dei buckets Zj,s =(Zjs1,...Z_jsm....Z_j,sTmax) di a_j che al tempo t sono stati visitati in posizione m, more formally: Zj,s appartiene Aj,m(t) if t>s+m-1.  




\begin{algorithm}[H]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{bucket size $T_{max}$}
			
			
			\Function{update}{$T_{max}$}
			\For{each arm $a_j \in  A$}
			\For{$m \in \{1, \ldots,T_{max}\}  $}
			
			\State compute $\overline{r}_{j,m}$ over the available buckets \;
			\State$c_{j,m}\gets R_j \sqrt{{2\ln(tT_{max}^{\frac{1}{4}})}/{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j \gets \sum_{m=1}^{T_{max}} \min(R_j,\overline{r}_{j,m}+c_{j,m}) $\;	
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}







\section{Bayesian Algorithms}

\section{Farsighted versions}

\section{Adaptive versions}























































\iffalse
	contenuto...




\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
			\Statex
			\Function{policy}{} 
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
				\State play arm $a_t$\;
				\State $\textsc{update}$
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
				\State play arm $a_z$ such that  $z = argmax_j$ $u_j (t)$\;
				\State $\textsc{update}$
			\EndFor
			\EndFunction
			\Statex
			\Function{update}{}
			\For{each arm $a_j \in  A$}
				\For{each bucket $b \in B_j $}
					\If{$b$ is ended}
						\State$x \gets   R_j * \sum_{m=1}^{Tmax}r_{b,m} $\;
						
						\State$\hat{\mu}_j(t) \gets \frac{\hat{\mu}_j(t-1)*(T_j(t)-1)+x}{T_j(t)}$\;
						\State$B_j \gets B_j \setminus \{b\}$\;	
										
					\EndIf				
				\EndFor			
				
			\State$c_j(t)\gets R_j * Tmax * \sqrt{\frac{2log(t)}{T_j(t)}}$\;
			\State$u_j(t) \gets  \hat{\mu}_j(t) +  c_j(t)$
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}


\section{Bound1}
\begin{itemize}
	\item $v_{j,m}(t)$ is the number of time an arm $a_j$ visited a bucket at istant $m$ up to time $t$
	
\end{itemize}
\begin{algorithm}[!h]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
	
			
			\Function{update}{}
			\For{each arm $a_j \in  A$}
			\For{$m \in {1, \ldots,Tmax}  $}
				
				\State$\hat{\mu}_{j,m}(t)  \gets (\sum_{b \in B_j\, visistable\, up \,to\, m}{}{r_{b,m}})/ v_{j,m}(t)$\;
				\State$c_{j,m}(t)\gets  \sqrt{\frac{2log(t*Tmax^{\frac{1}{4}})}{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j(t) \gets  R_j *\sum_{m=1}^{Tmax} min(1,\hat{\mu}_{j,m}(t)+c_{j,m}(t)) $\;	
			\EndFor
			
	
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}

\fi
