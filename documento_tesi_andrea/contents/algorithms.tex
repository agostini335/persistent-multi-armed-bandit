\chapter{Novel Algorithms}\label{C10}



\section{Grounding Concepts}
Multi-armed Bandit problems pertain to Online Learning, which means that in order to evaluate algorithms we need to design a simulation environment where we can mimic real-scenarios dynamics.
From a practical perspective we need a data structure which represents the concept of persistent reward. In order to do that we define a vector called \emph{Bucket} aimed to contain the rewards obtained at each time steps. With $r_{b,m}$ we refer to the reward at position $m$ on bucket $b$. The \emph{Bucket} has a fixed size \emph{Tmax}. During the simulation, the bucket will be parsed according to the experiment time, meaning that the learner can only visit the bucket-cells containing the rewards of the past. When at time $t$ an arm $a_j$ is played, the environment generates a bucket $b_{t,j}$ that is collected by the learner. We call $B_j$ the set of buckets collected after playing $a_j$.
The algorithms described below have a structural difference from the standard ones designed for traditional Multi-armed Bandit, infact in the presented framework more than one arms can have a set of active buckets (not totally parsed yet) simultaneously.  This parallelism implies that at each time instant we need to update the terms of every arms and not only of the last one played.


\section{Baseline UCB}
The pseudo-code of the Baseline UCB is depicted in Algorithm1. This algorithm extends the idea of the well known UCB algorithm in the case of persistent rewards. The reward is considered as a unique variable available after the termination of the bucket. The first k istants form the initialization phase, during which  each arm is chosen once. After the k-th time istant the loop phase begins. Here the agent plays the arm having the largest index $u_j(t)$, the upper confidence bound of the arm $a_j$ at time t. It is the sum of  the empirical reward of the arm measured so far $\hat{\mu}_j(t) $ and the exploration term $c_j(t) $. After the play of an arm, the \emph{Update Procedure} occurs. For each arm it scans every bucket collected and when a terminated bucket is detected (after $Tmax$ istants from his generation) a new reward is observed. $T_j(t)$ representes the number of times arm $a_j$ has been pulled up to time $t$. $R_j$ is the reward of the arm $a_j$ (SISTEMARE).




\begin{algorithm}[!h]
	\caption{\texttt{Baseline UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
			\Statex
			\Function{policy}{} 
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
				\State play arm $a_t$\;
				\State $\textsc{update}$
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
				\State play arm $a_z$ such that  $z = argmax_j$ $u_j (t)$\;
				\State $\textsc{update}$
			\EndFor
			\EndFunction
			\Statex
			\Function{update}{}
			\For{each arm $a_j \in  A$}
				\For{each bucket $b \in B_j $}
					\If{$b$ is ended}
						\State$x \gets   R_j * \sum_{m=1}^{Tmax}r_{b,m} $\;
						
						\State$\hat{\mu}_j(t) \gets \frac{\hat{\mu}_j(t-1)*(T_j(t)-1)+x}{T_j(t)}$\;
						\State$B_j \gets B_j \setminus \{b\}$\;	
										
					\EndIf				
				\EndFor			
				
			\State$c_j(t)\gets R_j * Tmax * \sqrt{\frac{2log(t)}{T_j(t)}}$\;
			\State$u_j(t) \gets  \hat{\mu}_j(t) +  c_j(t)$
			\EndFor
			
			
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:BaselineUCB}
\end{algorithm}


\section{Bound1}
\begin{itemize}
	\item $v_{j,m}(t)$ is the number of time an arm $a_j$ visited a bucket at istant $m$ up to time $t$
	
\end{itemize}
\begin{algorithm}[!h]
	\caption{\texttt{Bound1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]
			
			
			\Require{arm set $A = \{a_1, a_2, \ldots, a_k\}$, time horizon $N$, bucket size $Tmax$}
			\Statex
			\Function{policy}{} 
			\For { $t \in  \{1, \ldots,k\}$} \Comment{init phase}
			\State play arm $a_t$\;
			\State $\textsc{update}$
			\EndFor
			\For { $t \in  \{k+1, \ldots,N\}$} \Comment{loop phase}
			\State play arm $a_z$ such that  $z = argmax_j$ $u_j (t)$\;
			\State $\textsc{update}$
			\EndFor
			\EndFunction
			\Statex
			
			
			\Function{update}{}
			\For{each arm $a_j \in  A$}
			\For{$m \in {1, \ldots,Tmax}  $}
				
				\State$\hat{\mu}_{j,m}(t)  \gets (\sum_{b \in B_j\, visistable\, up \,to\, m}{}{r_{b,m}})/ v_{j,m}(t)$\;
				\State$c_{j,m}(t)\gets  \sqrt{\frac{2log(t*Tmax^{\frac{1}{4}})}{v_{j,m}(t)}}$\;
			
			\EndFor		
			
			\State $u_j(t) \gets  R_j *\sum_{m=1}^{Tmax} min(1,\hat{\mu}_{j,m}(t)+c_{j,m}(t)) $\;	
			\EndFor
			
	
			
			
			\EndFunction
			
		\end{algorithmic}
	\end{scriptsize}
	\label{alg:Bound1}
\end{algorithm}



