\chapter{Preliminaries}\label{CPRELIMINARIES}

In this chapter, we present the standard Multi-Armed Bandit problem and the grounding concepts of the thesis by reviewing the relevant literature. In the first section we present the motivating exploration-exploitation dilemma. Then, we give a formal definition of the Multi-Armed Bandit problem and we show the most important policies. Finally, in the last section, we introduce the concept of persistent reward and we make some observations regarding the state-of-the-art.

\section{The Exploration-Exploitation Dilemma}

A popular disease does not have certified treatments yet, to treat patients suffering from it a doctor has to choose among two experimental therapies: a red pill and a blue one. So far, the doctor has treated a total of ten patients, giving the red pill to some of them and the blue pill to the others. Some patients healed and some did not, as shown in figure \ref{f:clinical}.

\begin{figure}[t!]
	\centering
	\begin{tabular}{|c|l|lcccccccccc|}
		\hline
		\multicolumn{3}{|c}{\textbf{Patients}} & 1      & 2      & 3      & 4      & 5      & 6      & 7      & 8      & 9      & 10     \\ \hline
		\multicolumn{3}{|c}{Red Pill}          & \xmark &        &        & \xmark &        & \cmark & \cmark & \cmark & \xmark & \xmark \\
		\multicolumn{3}{|c}{Blue Pill}         &        & \cmark & \xmark &        & \xmark &        &        &        &        &        \\ \hline
	\end{tabular}
	
	\caption{Therapy assignment to the first ten patients. Those marked with \cmark healed, the others marked with \xmark  did not. }
	\label{f:clinical}
\end{figure}
The red pill seems to perform slightly better than the blue one since it has an efficacy around 43\% (three out of seven patients healed) while the other 33\% (one out of three patients healed). Which strategy should the doctor follow to treat the next patients? Is it optimal to assign the red pill ignoring the blue one? Or the poor efficacy of the blue pill is due to chance and the doctor has to try it a few more times? How many more times? This illustrates the so-called 	\emph{exploration-exploitation} dilemma. If the doctor will assign the red pill to the next patient we say that he/she is \emph{exploiting} the knowledge acquired so far, since, apparently, the red pill is performing better. On the other hand, if the doctor will assign the blue pill to the next patient we say that he/she is \emph{exploring} the other therapy to acquire new knowledge, to have a better understanding of the overall situation and possibly to discover that the blue pill is better than the red one. The dilemma is that neither exploration nor exploitation can be pursued exclusively to find the optimal action. The doctor must try both the available therapies, managing the trade-off between exploration and exploitation, progressively favoring the one that appear to be best. The exploration–exploitation dilemma has been intensively studied by mathematicians for many decades, yet it remains unresolved \citep{sutton2018reinforcement}. Finding the right balance between exploration and exploitation is at the heart of the Multi-Armed Bandit problem presented in the following section.

\section{The Multi-Armed Bandit Problem}
Consider the following learning problem. You are faced repeatedly with a choice among $K$ different  actions. After each choice you receive a numerical reward generated by the environment you are interacting with. Your objective is to accumulate as much reward as possible over some time period, for example, over $1000$ action selections, or \emph{time instants}. In a
casino, this kind of sequential decision making problem is obtained when a player is
facing many slot machines at once (a “multi-armed bandit”), and must
repeatedly choose where to insert the next coin. The name \emph{bandit} refers to the colloquial
term for a slot machine (“one-armed bandit” in American slang).
There are three fundamental formalizations of the bandit problem
depending on the assumed nature of the reward process: stochastic, adversarial \citep{Auer1995Adversarial}, and Markovian \citep{Anantharam1987Markovian}. In this thesis, we focus our attention on the stochastic setting that was first studied by \cite{Robbins1952sequential}. In a stochastic \emph{Multi-Armed Bandit} (MAB) problem, at each time instant $t$, an agent must pull (choose) an arm (action) $a_j$ from an arm set $A = \{1,\dots,K\}$. Pulling an arm $a_j$ at time $t$ produces a reward drawn from an unknown distribution $v_j$ with unknown expectation $\mu_j$. The rewards are defined by random variables $X_{j,t}$ for $1\leq j \leq K$ and  $ t \geq 1$, where each $j$ is the index of an arm. Successive pulls of arm $a_j$ yield rewards $X_{j,t_1},X_{j,t_2},\dots$ which are independent and identically distributed random variables with unknown expectation $\mu_j \in (0,1)$. The goal of an agent acting in this environment is to maximize the cumulative reward of its action after $n$ time steps, i.e., $\sum_{t=1}^{n} X_{a_t,t}$, where $a_t$ is the arm pulled at time $t$. The number of time instants $n$ is called \emph{time horizon}. At each time instant an agent faces the exploration-exploitation dilemma since he/she has to choose between the current best arm (exploitation) or trying other alternatives to have a better knowledge of the environment, hoping to discover a better arm (exploration). To achieve the goal of maximizing the cumulated reward, an agent follows a \emph{policy}, that is, an algorithm that chooses the next arm to pull based on the sequence of past pulls and obtained rewards. In the following subsections we present three of the most important policies for the stochastic Multi-Armed Bandit problem.

\subsection*{UCB1}
The policy UCB1 has a frequentist approach and it is based on the principle of \emph{optimism in the face of uncertainty}. This principle states that one should act as if the environment is as nice as plausibly possible. For MAB, this means using the data observed so far to assign each arm a value, called \emph{upper confidence bound} that with high probability it is an overestimate of the unknown mean. An agent that follows the UCB1 policy,  at each time instant $t$, plays the arm $a_j$ that has the largest upper confidence bound $u_j(t)$. The upper confidence bound $u_j(t)$ is computed as the sum of two terms: the empirical mean reward obtained so far $\hat{\mu}_j(t)$ and the uncertainty of the estimate, $c_j(t)$. Essentially, the empirical mean reward $\hat{\mu}_j(t)$ represents the exploitation term of an arm: it says exactly what we obtained so far, when the arm is played it increase or decrease according to the obtained reward. On the other hand, the uncertainty $c_j(t) = \sqrt{\frac{2\ln t}{T_j(t)}}$, where  $T_j(t)$ is the number of times that the arm $a_j$ has been pulled up to time $t$, gives a contribution in term of exploration since it increases slowly as long as the arm is not pulled, then, as soon as the arm is pulled, it reduces more drastically.  The pseudo-code of the UCB1 policy is depicted in Algorithm \ref{a:ucb1}. The first $K$ instants form the initialization phase where each arm is played once. 

\begin{algorithm}[H]
	\caption{\texttt{UCB1}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\For { $t \in  \{1, \ldots,K\}$} \Comment{init phase}
			\State pull arm $a_t$ 
			\EndFor
			\For { $t \in  \{k+1, \ldots,n\}$} \Comment{loop phase}
			\For { $j \in  \{1, \ldots,K\}$}
			\State compute $\hat{\mu}_j(t)$
			\State $c_j(t) \gets \sqrt{\frac{2\ln t}{T_j(t)}}$
			\State $u_j(t) \gets \hat{\mu}_j(t) + c_j(t)  $
			
			\EndFor
			\State pull arm $a_i$ such that  $i = \argmax_j$ $u_j(t)$
			\EndFor	
		\end{algorithmic}
	\end{scriptsize}
	\label{a:ucb1}
\end{algorithm}

\cite{auer2002finite} provided a theoretical guarantee for the policy UCB1. Given any sub-optimal arm $a_j$, it has been proved an upper bound on $\mathbb{E}[T_j(n)]$, namely, the expected number of times a policy pulls the sub-optimal arm $a_j$ up to time $n$.
\begin{theorem} If policy UCB1 is run over a stochastic MAB setting, the expected number of pulls of a sub-optimal arm $a_j$ after $n$ runs is at most:
	$$\mathbb{E}[T_j(n)] \leq \frac{8 \ln n}{\Delta_j^{2}} + 1 +\frac{\pi^{2}}{3} ,$$
where  $\Delta_j = \mu^{*}-\mu_j$ and $\mu^{*}$ is the expected reward of the optimal arm.
	
\end{theorem}




 
\subsection*{Thompson Sampling}
Thompson Sampling (TS) is a policy that adopts a Bayesian approach. It was introduced by \cite{thompson1933likelihood} for the Bernoulli Bandit, and generalized by \cite{agrawal2012analysis}  for the general stochastic Bandit. The basic idea is to assume a simple prior distribution on the parameters of the reward distribution of every arm, and at any time instant, play an arm according to its posterior probability of being the best arm. For simplicity, we provide the details of the Thompson Sampling algorithm for the Bernoulli Bandit, i.e. when the rewards are either 0 or 1, and for arm $a_j$ the probability of success (reward = 1) is $\mu_j$. We assume Beta distribution as priors because it turns out to be a very convenient choice in this setting. Indeed, Beta distribution is useful for Bernoulli rewards because if the prior is a Beta($\alpha$, $\beta$) distribution, then after observing a Bernoulli trial, the posterior distribution is Beta($\alpha +1$, $\beta$) or Beta($\alpha$, $\beta +1$), depending on whether the trial resulted in a success or failure, respectively. The TS algorithm initially assumes arm $a_j$ to have prior Beta($1$,$1$) on $\mu_j$, which is equivalent to the uniform distribution. At each time $t$, having observed $S_j(t)$ successes and $F_j(t)$ failures, the distribution on $\mu_j$ are updated as Beta($S_j(t)$,$F_j(t)$). Then, for each arm $a_j$, it is sampled $\theta_j$ from the posterior distributions of $\mu_j$, and the arm having the largest $\theta_j$ is pulled. The exploration in Thompson Sampling comes from the randomisation. If the posterior is poorly concentrated, then the fluctuations in the samples are expected to be large and the policy will likely explore. On the other hand, as more data is collected, the posterior concentrates towards the true value and the rate of exploration decreases. The randomness of TS complicates a lot the theoretical analysis and a logarithmic upper bound on $\mathbb{E}[T_j(n)]$, the expected number of times a policy pulls the sub-optimal arm $a_j$ up to time $n$, has been proved by \cite{agrawal2012analysis} years later the original formulation. The pseudo-code of the TS policy is depicted in Algorithm \ref{a:TS}.


\begin{algorithm}[H]
	\caption{\texttt{Thompson Sampling}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\For { $j \in  \{1, \ldots,K\}$} \Comment{init phase}
			\State $S_j \gets 1 $, $F_j \gets 1 $
			\EndFor
			\For { $t \in  \{1, \ldots,n\}$} \Comment{loop phase}
			\For { $j \in  \{1, \ldots,K\}$}
			
			\State sample $\theta_j(t)$ from the Beta($S_j$,$F_j$) distribution
					
			\EndFor
			\State pull arm $a_i$ such that  $i = \argmax_j$ $\theta_j(t) $ and observe reward ${r_t}$	
			\State 	\textbf{if} $r_t=1$  \textbf{then} $S_j \gets S_j + 1$ \textbf{else}  $F_j \gets F_j + 1$  \textbf{end if}
			
			
			\EndFor	
		\end{algorithmic}
	\end{scriptsize}
	\label{a:TS}
\end{algorithm}


\subsection*{Bayes-UCB}
Bayes-UCB is a Bayesian algorithm  that could be seen as a variant of Thompson Sampling without randomness. Indeed, it eliminates the randomness present in TS by using the quantiles of the posterior distributions on $\mu_j$ rather than drawing a sample from them. More precisely, let $Q(1-\alpha,\rho)$, with $0< \alpha<1$, be the quantile function associated to the distribution $\rho$ such that: $ \mathbb{P}_\rho(X\leq Q(1-\alpha,\rho))=1-\alpha$. At each time instant $t$, for each arm $a_j$, the Bayes-UCB algorithm computes an index $q_j(t)$ in the following way: \begin{center}
$ q_j(t) = Q(1-\frac{1}{t(\log n)^c},\rho_j(t)) ,$
\end{center}where $\rho_j(t)$ is the distribution associated to $\mu_j$ at time $t$, $n$ is the time horizon and $c$ is a parameter. The arm $a_j$ with the largest index $q_j(t)$ is pulled and the distribution on $\mu_j$ updated according to the reward received. In the derivation of theoretical guarantees the parameter $c$ is set to $c>=5$ but it is set $c=0$ to achieve better experimental results. We can see that Bayes-UCB, at each time $t>1$, for each arm $a_j$, selects an upper bound of the mean adopting the principle of optimism in the face of uncertainty. Similar to the policy UCB1 described above, as long as an arm is not pulled its upper bound grows. We provide the pseudo-code of the policy in Algorithm \ref{a:BU} for the case of Bernoulli Bandit with Beta distribution as prior. \cite{kaufmann2012bayesian} proposed the Bayes-UCB algorithm and provided a logarithmic upper bound on $\mathbb{E}[T_j(n)]$, the expected number of times a policy pulls the sub-optimal arm $a_j$ up to time $n$.

\begin{algorithm}[H]
	\caption{\texttt{Bayes-UCB}}
	\begin{scriptsize}
		\begin{algorithmic}[1]						
			\For { $j \in  \{1, \ldots,K\}$} \Comment{init phase}
			\State $S_j \gets 1 $, $F_j \gets 1 $
			\EndFor
			\For { $t \in  \{1, \ldots,n\}$} \Comment{loop phase}
			\For { $j \in  \{1, \ldots,K\}$}
			
			\State  $q_j(t) \gets$  $Q(1-{1}/{t(\log n)^c},Beta(S_j,F_j))$
			
			\EndFor
			\State pull arm $a_i$ such that  $i = \argmax_j$ $q_j(t) $ and observe reward ${r_t}$	
			\State 	\textbf{if} $r_t=1$  \textbf{then} $S_j \gets S_j + 1$ \textbf{else}  $F_j \gets F_j + 1$  \textbf{end if}
			
			
			\EndFor	
		\end{algorithmic}
	\end{scriptsize}
	\label{a:BU}
\end{algorithm}


\section{Introducing Persistent Rewards}

The motivation behind this thesis can be resumed in one straightforward question: what does happen when the reward is not a number immediately available, but it is spread over the time following the pull of an arm? This is the case of many real-world scenarios as described in Chapter \ref{C_INTRO} or in the Examples \ref{magazine}, \ref{trial}, \ref{cloud} provided in Chapter \ref{CF}. The aim of our thesis is to study the  Multi-Armed Bandit problem with \emph{persistent reward}, where with the term persistent reward we are referring to a reward that is persistently gained by an agent for a certain timespan after an arm has been pulled. From a certain perspective, one could consider the reward as a single variable available after the reward acquisition process is terminated. This consideration reduces the persistent reward problem to a delayed reward problem. MAB problem with delayed reward has been extensively studied in the literature. Our major interest is to design methodologies which exploit the partial information during the reward acquisition process without the need to wait its termination to see the reward associated to the pull of an arm. Let us consider a subscription business pricing problem where a seller, that repeatedly signs new contracts, wants to discover which is the monthly price to assign to his/her service to maximize the total reward. At this purpose, we want to develop algorithms that constantly take in consideration the monthly payments made by the user, avoiding to wait the end of the contract to see the total reward acquired thanks to that contract. To the best of our knowledge no previous study addressed the Multi-Armed Bandit problem with persistent reward depicted so far. Standard MAB methodologies can not be directly used in the persistent reward setting. For this reason, we provide the formalization of the Multi-Armed Bandit problem with persistent reward (MAB-PR) and we propose new algorithms specifically designed to tackle this problem.

\section{Related works}
An overwhelming majority of the literature focuses on scenarios that assume instantaneous rewards.
Even such a simple model have been successfully applied to a wide range of application scenarios, like advertising allocation problems \cite{gatti2015truthful,nuara2020online,nuara2018combinatorial}, dynamic pricing \cite{trovo2018improving,trovo2020sliding}, security scenarios \cite{bisi2017regret}, and network routing \cite{li2016contextual}.
Nonetheless, in most common real-world problem the feedback is not provided as the learner performs the action, but it is provided with a given delay, e.g., \cite{nuara2019dealing}, or provided over a timespan, as mentioned above.


In the Bayesian MAB setting, the problem of delayed feedback was first proposed by Anderson \cite{anderson1964sequential}. \cite{joulani2013} studied Multi-Armed Bandit problem under delayed feedback and proposed the Delayed-UCB1 algorithm. Delayed-UCB1 is modification of the standard UCB1 algorithm, depicted in Algorithm \ref{a:ucb1}, adapted to be feed with delayed rewards. After this work, the delayed setting has been extended to more complex bandit scenarios, \emph{e.g.}, linear, contextual, non-stationary bandit under delayed feedbacks and/or rewards \citep{vernade2020non, vernade2020linear, gael2020stochastic, pike2018bandits}.

Another set of techniques are interested in solving delayed feedback problems in specific real-world scenarios.
For instance, a set of techniques are focused in solving the so called dynamic pricing problem.
More specifically, each available price is an arm of a bandit, and the goal is to sell multiple units of the same item maximizing the reward gained in the process.
Dynamic pricing has been studied as a bandits problem in the literature for both the stochastic \cite{babaioff2015dynamic} and the adversarial setting \cite{amin2013learning}.
These works overcome the classical assumption in the economic theory for which the demand curve is known, using an online approach in which the curve is learned using the interaction with the buyer, e.g., holding a posted price auction \citep{babaioff2017posting,romano2020online} to determine the optimal price for the good.

Another significant example is the Interned advertising problem, in which bandits techniques are widely applied \citep{badanidiyuru2013bandits, nuara2018combinatorial, avadhanula2021stochastic}. In this setting, the delayed feedback comes from the fact that clicks can be observed within a few seconds after an ad display but the corresponding sale, if any, will take hours to happen.
This has been solved with a specific delay-feedback algorithm in \citet{vernade2017stochastic}.

Notice that, to the best of our knowledge, there is not work specifically addressing the setting in which the reward of a single pull of an arm spreads over multiple time instants.
